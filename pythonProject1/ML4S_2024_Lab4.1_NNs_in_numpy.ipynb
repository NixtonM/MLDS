{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8b0178-7212-493e-9681-3f717163eb88",
   "metadata": {},
   "source": [
    "# LAB-4.1: Neural Networks in Numpy\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this lab session, we investigate a regression task by a basic deep learning architecture. You need to implement the neural network from scratch by using only the numpy package. \n",
    "\n",
    "### General Announcements\n",
    "\n",
    "* The exercises on this sheet are graded by a maximum of **20 points**. You will be asked to implement several functions.\n",
    "* Team work is not allowed! Everybody implements his/her own code. Discussing issues with others is fine, sharing code with others is not. \n",
    "* If you use any code fragments found on the Internet, make sure you reference them properly.\n",
    "* You can send your questions via email to the TAs until the deadline."
   ]
  },
  {
   "cell_type": "code",
   "id": "6f76402f-b4db-4a51-8ac2-5d4d54e6f7ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T09:12:34.668370Z",
     "start_time": "2024-04-24T09:12:34.273420Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6c49288d-0404-401e-b789-36d86e7c8449",
   "metadata": {},
   "source": [
    "# 1) Generating a Toy Dataset\n",
    "- Generate a dataset of random values with the help of numpy. (<span style=\"color:green\">2 points</span>)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1102c58-0be3-4d66-8a6a-190ea101e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T09:14:52.201655Z",
     "start_time": "2024-04-24T09:14:52.196190Z"
    }
   },
   "source": [
    "def generate_data(num_samples: int, num_features: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "        - Number of Samples (dtype: Integer)\n",
    "        - Number of dimension in Features/Data (dtype: Integer)\n",
    "    Outputs:\n",
    "        - data (numpy.ndarray | dtype: numpy.float | Shape=(num_sample, num_feature))\n",
    "        - labels (numpy.ndarray | dtype: numpy.float | Shape=(num_sample))\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # Keep the seed the same\n",
    "    data = np.random.randn(num_samples, num_features)\n",
    "    labels = np.random.randn(num_samples)\n",
    "    return data, labels\n",
    "\n",
    "X, Y = generate_data(1, 10)\n",
    "print(X.shape, Y.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (1,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c3e0142d-28cb-479d-bbc8-46914075ad72",
   "metadata": {},
   "source": [
    "# 2) Designing Deep Learning Modules from Scratch\n",
    "- All modules in a deep learning models have a common structure. You can find it below."
   ]
  },
  {
   "cell_type": "code",
   "id": "af98e9f1-c38d-4bec-b729-e8df1fefe084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T09:15:16.026575Z",
     "start_time": "2024-04-24T09:15:16.021658Z"
    }
   },
   "source": [
    "# DO NOT MODIFY\n",
    "class GenericModule:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pass a tensor through the module. This is called a forward pass.\n",
    "        \n",
    "        :param x: Input tensor.\n",
    "        :returns: Output of the layer after applying it to the input tensor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the gradients of all trainable weights in the layer based on the gradient information coming in from the next layer.\n",
    "        Then calculate the gradient being passed down to the previous layer.\n",
    "        This is called a backward pass.\n",
    "        \n",
    "        The gradient basically tells the layer in which direction it should adapt its outputs to reduce the loss.\n",
    "        The layer then adapts its weights to increase the outputs towards the direction dictated by the gradient.\n",
    "        \n",
    "        :param grad: Gradient information derived from the next layer.\n",
    "        :returns: Gradient information for the previous layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "e8ccf84a-ff47-460f-ba9a-5873bddcce2c",
   "metadata": {},
   "source": [
    "- Now, you need to implement the module by using numpy functions only.\n",
    "    - Implement ReLU, which is the function: $f(x) = max(0, x)$ (<span style=\"color:green\">1 point</span> for forward & <span style=\"color:green\">1 point</span> for backward)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b98c8f77-21a4-4f9f-9a45-d51554aff3e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T09:35:16.141415Z",
     "start_time": "2024-04-24T09:35:16.134209Z"
    }
   },
   "source": [
    "class ReLU(GenericModule):\n",
    "    def __init__(self, name, shape):\n",
    "        self.name = name\n",
    "        self.non_zero_index = np.zeros(shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # for the backpropagation you need to remember which indexes were not masked by the ReLU\n",
    "        out = np.maximum(0, x)  \n",
    "        self.non_zero_index[:] = 1\n",
    "        self.non_zero_index[x < 0] = 0\n",
    "        return out\n",
    "    # \n",
    "    # def backward(self, grad):\n",
    "    #     return ?\n",
    "\n",
    "# Checking Module - DO NOT MODIFY\n",
    "obj = ReLU('Temporary', 10)\n",
    "output = obj.forward(X[0] - X[0].mean())\n",
    "df_temp = pd.DataFrame({'input': X[0] - X[0].mean(), 'output': output, 'gradient': obj.non_zero_index})\n",
    "print(df_temp)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      input    output  gradient\n",
      "0  0.048653  0.048653       1.0\n",
      "1 -0.586325  0.000000       0.0\n",
      "2  0.199627  0.199627       1.0\n",
      "3  1.074969  1.074969       1.0\n",
      "4 -0.682214  0.000000       0.0\n",
      "5 -0.682198  0.000000       0.0\n",
      "6  1.131152  1.131152       1.0\n",
      "7  0.319374  0.319374       1.0\n",
      "8 -0.917535  0.000000       0.0\n",
      "9  0.094499  0.094499       1.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "fe8d47e4-7bcb-4850-8c19-8ddaccfea3d5",
   "metadata": {},
   "source": [
    "- Implement dense layer: $f(x) = (w \\cdot x) + b$ (<span style=\"color:green\">1 point</span> for initialization & <span style=\"color:green\">1 point</span> for forward & <span style=\"color:green\">2 points</span> for backward)\n",
    "- Initialize the weights randomly in the range [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68419a30-dfcb-47d0-bda5-14fec57d1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(GenericModule):\n",
    "    def __init__(self, name, in_size, out_size):\n",
    "        self.name=name\n",
    "        self.w = np.random.uniform(low=-1, high=1, shape=(in_size, out_size))\n",
    "        self.b = np.random.uniform(low=-1, high=1, shape=(in_size, out_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = ?\n",
    "        self.x = x.reshape(-1,1)  # need to save x for backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad_x = ?\n",
    "        self.grad_w = ?\n",
    "        self.grad_b = ?\n",
    "        self.grad_b = self.grad_b.reshape(-1)  # needed so you don't get shape missmatches\n",
    "        return grad_x\n",
    "\n",
    "# Checking Module - DO NOT MODIFY\n",
    "obj = Dense('Temporary', 10, 32)\n",
    "output = obj.forward(X[0] - X[0].mean())\n",
    "print(output.shape, obj.w.shape, obj.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510683b-f3fa-4a7b-a206-54bd12b22bda",
   "metadata": {},
   "source": [
    "- Implement the squared error loss function: $L(x, y) = |x - y|_2^2$ (<span style=\"color:green\">1 point</span> for forward & <span style=\"color:green\">1 point</span> for backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac89773-c48d-4dd3-8b76-926e7f9c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_loss(GenericModule):\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        :params x: Prediction for a single sample.\n",
    "        :params y: Target of the sample.\n",
    "        \"\"\"\n",
    "        self.y = y  # need to save for backward\n",
    "        self.x = x  # need to save for backward\n",
    "        out = ?\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        grad = ?\n",
    "        return grad.reshape(1,1)\n",
    "    \n",
    "# Checking Module\n",
    "obj = SE_loss()\n",
    "output = obj.forward(np.array([1]), np.array([10]))\n",
    "print(output, obj.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf7d1b-22cf-43e7-9dea-63437c698948",
   "metadata": {},
   "source": [
    "# 3) Training a Neural Network\n",
    "- Define an architecture as a list by using dense and relu layers (<span style=\"color:green\">2 points</span>)\n",
    "    - Use the given hyper-parameters during designing: \n",
    "        1) Dense (??, 32)\n",
    "        2) ReLU (??)\n",
    "        3) Dense (??, 1)\n",
    "    - Re-generate the dataset\n",
    "        - Number of Samples = 15\n",
    "        - Number of Features = 10\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb376c27-b5bf-48d2-b7ee-24e1dc4fc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ?\n",
    "X, Y = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70338330-81a8-4a03-bc77-95fb3ed03e2d",
   "metadata": {},
   "source": [
    "- Forward each sample through the network and calculate the loss (<span style=\"color:green\">2 points</span>)\n",
    "- Run the backwards pass and implement the gradient decent algorithm for the dense layers (<span style=\"color:green\">2 points</span>)\n",
    "- Note that you need to rerun the previous cell to reset the network before training again\n",
    "- Note that we are currently not using batching, and instead only pass one sample through the network each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648b372-8d14-4bd2-a4aa-1e9ec2f5d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Paraters for Training\n",
    "learning_rate = 0.01\n",
    "num_epoch = 10\n",
    "loss = SE_loss()\n",
    "\n",
    "print(\"-------Network-------\")\n",
    "for layer in net:\n",
    "    print(layer.name)\n",
    "    if isinstance(layer, Dense):\n",
    "        print(\"   Weight shape:\", layer.w.shape)\n",
    "        print(\"   Bias shape:\", layer.b.shape)\n",
    "        \n",
    "print(\"\\n-------Training-------\")\n",
    "\n",
    "# Training Phase\n",
    "for epoch_no in range(num_epoch):\n",
    "    \n",
    "    cumulative_loss = 0\n",
    "    \n",
    "    for sample_idx in range(len(X)):\n",
    "        \n",
    "        x = ?\n",
    "        y = ?\n",
    "\n",
    "        # Forward Pass\n",
    "        for layer in net:\n",
    "            x = ?\n",
    "\n",
    "        loss_val = ?\n",
    "        cumulative_loss += loss_val\n",
    "\n",
    "        # Backward Pass\n",
    "        grad = ?\n",
    "        for layer in net[::-1]:\n",
    "            grad = ?\n",
    "            if isinstance(layer, Dense):  # Optimization: Gradient Decent\n",
    "                layer.w = ?\n",
    "                layer.b = ?\n",
    "                \n",
    "    print(\"Epoch:\", epoch_no, \"- Loss:\", cumulative_loss / len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2254877-e6a1-4232-a4fb-459e541779b7",
   "metadata": {},
   "source": [
    "- Right now the training is being tracked with ugly long numbers. Write a function that plots the loss curve nicely instead. (<span style=\"color:green\">1 point</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d0da2-608b-444e-9c96-d9e65a3c7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_vals):\n",
    "    # Hint: Use matplotlib.pyplot\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "plot_loss_curve(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0039c-a59e-4378-a0de-83e3d4e8549d",
   "metadata": {},
   "source": [
    "- Your model should currently reach a performance between 0.05 and 0.08 after the first 10 epochs\n",
    "- Try out multiple learning rates and find one that provides a loss < 0.04 after the first 10 epochs (<span style=\"color:green\">1 point</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d184f-1f89-46ef-be45-1d9c84f29ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceec632-8292-427c-90a7-1b8e693b365f",
   "metadata": {},
   "source": [
    " - Name at least two important parts of a deep learning pipeline that we didnÂ´t do in this notebook. (<span style=\"color:green\">2 points</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d2a72-5d62-4247-a5b2-38802ba1414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
